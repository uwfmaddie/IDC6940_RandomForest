---
title: "Random Forest in the Health Industry - Spring 2025"
subtitle: "An analysis of the random forest algorithm and its applications in the health industry"
author: "Maddie Sortino and Jisa Jose (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

Literature Review: [litreview.html](litreview.html)

## Introduction

Machine learning has significantly advanced predictive analytics,
particularly in the medical industry and clinical decision-making. Among
the many available algorithms, Random Forest (RF) has emerged as a
powerful tool due to its ability to handle high-dimensional data,
resistance to overfitting, and high accuracy in predicting medical
events [@rigatti2017random]. RF is an ensemble
learning method composed of multiple decision trees, which are generated
through bagging and random feature selection. The combined efforts of
these trees in bootstrap aggregation allow for superior classification
and regression predictions compared to classical statistical models
[@biau2016random]. This flexibility enables
biomedical experts to tackle various tasks, including cancer survival
analysis, disease progression prediction, and healthcare resource
optimization.

One of the most notable applications of RF in healthcare is survival
analysis, particularly in colon cancer research. Studies utilizing SEER
data have compared RF to the Cox proportional hazards model,
highlighting RF’s ability to handle missing data and complex
interactions more effectively [@breiman2001random]. Additionally, RF has
been widely used in clinical decision-making, such as predicting ICU
patient outcomes and identifying those at high risk of sepsis.Another critical application is diabetes prediction and prevention. RF not only forecasts diabetes development but also provides personalized recommendations to healthcare professionals, helping them implement preventive measures and improve patient outcomes[@khine2022diabetes].

Beyond diagnostics, RF is also instrumental in handling imbalanced
datasets. For instance, in predicting disease susceptibility, random
subsampling techniques within RF have been shown to outperform other
machine learning models such as support vector machines, boosting, and
bagging [@khalilia2011predicting]. Furthermore, RF has been used in
healthcare resource planning, such as forecasting the demand for
essential medications in public health facilities to ensure supply chain
efficiency and prevent shortages [@mbonyinshuti2022demand].

RF is a powerful algorithm, but it also has its challenges.
Hyperparameter tuning is crucial for maximizing predictive accuracy,
with factors such as the number of trees (L), sample size per tree, and
the number of variables considered at each split (mtry) all
significantly influencing model performance [@probst2019hyperparameters]. While RF often performs well with default
settings, fine-tuning these parameters can enhance both reliability and
speed [@boulesteix2012overview]. However, the major drawback remains
model interpretability, which is critical in medical decision-making. Researchers have proposed methods such as conditional inference forests (CIF) to improve reliability while
reducing bias in variable selection [@dai2018using].

Since RF was first introduced to clinical diagnostics, it has been
regarded as one of the most effective tools in medical prediction
models. This study explores its performance compared to traditional
statistical techniques and evaluates whether advancements in
hyperparameter optimization can further enhance its effectiveness. By
synthesizing insights from multiple sources, this analysis provides a
comprehensive understanding of RF’s potential in healthcare analytics
while identifying areas for further refinement.

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

Packages

```{r}
library(readr)
library(caret)
library(ggplot2)
library(tidyverse)
library(gridExtra)
library(scales)
```

### Data Exploration and Visualization

Data Set: [Heart Failure Prediction
Data](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data)

This data set is a compilation of five different data sets from around
the world. It contains 11 features: age, sex, chest pain type, resting
blood pressure, cholesterol, fasting blood sugar, resting
electrocardiogram results, max heart rate, exercise-induced angina, old
peak, and slope of peak exercise ST segment. The data set is used to
predict whether the patient has heart disease or not.

**Column Descriptions:**<br> Age: age of the patient \[years\]<br> Sex:
sex of the patient \[M: Male, F: Female\]<br> ChestPainType: chest pain
type \[TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain,
ASY: Asymptomatic\]<br> RestingBP: resting blood pressure \[mm Hg\]<br>
Cholesterol: serum cholesterol \[mm/dl\]<br> FastingBS: fasting blood
sugar \[1: if FastingBS \> 120 mg/dl, 0: otherwise\]<br> RestingECG:
resting electrocardiogram results \[Normal: Normal, ST: having ST-T wave
abnormality (T wave inversions and/or ST elevation or depression of \>
0.05 mV), LVH: showing probable or definite left ventricular hypertrophy
by Estes' criteria\]<br> MaxHR: maximum heart rate achieved \[Numeric
value between 60 and 202\]<br> ExerciseAngina: exercise-induced angina
\[Y: Yes, N: No\]<br> Oldpeak: oldpeak = ST \[Numeric value measured in
depression\]<br> ST_Slope: the slope of the peak exercise ST segment
\[Up: upsloping, Flat: flat, Down: downsloping\]<br> HeartDisease:
output class \[1: heart disease, 0: Normal\]<br>

The figures below explore the data set to examine data types, results,
and distributions across the various features and classes.

```{r}
data <- read.csv('heart.csv')
#head(data,5)
```

```{r}
str(data)
```
Noted abnormalities to be addressed in cleaning stage: RestingBP = 0 and Cholesterol = 0
```{r}
summary(data)
```
Check for missing values - none found.
```{r}
colSums(is.na(data))
```

Distribution of Features
```{r, warning=FALSE, echo=T, message=FALSE}
# The distribution of 'Age' with a histogram - normal distribution
ageplot <- ggplot(data, aes(x = Age)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution", x = "Age", y = "Count")

# The distribution of 'Heart Disease' with a histogram - no class imbalance
hdplot <- ggplot(data, aes(x = HeartDisease)) +
  geom_bar(fill = "blue", color = "black") +
  theme_minimal() +
  scale_x_continuous(breaks=c(0,1)) +
  labs(title = "Heart Disease Class Distribution", x = "Heart Disease", y = "Count")

grid.arrange(ageplot, hdplot, ncol = 2)
```
### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
