---
title: " Literature Reviews"
subtitle: "Exploring the Applications of Random Forest in Healthcare"
author: "Jisa Jose, Maddie Sortino"
date: today
format:
  html: default
bibliography: references.bib
csl: "https://www.zotero.org/styles/apa"  # Use online CSL if local file is missing
self-contained: true
execute: {}
editor: {}
---

## Jisa's Literature Review

**Title:** Random Forests\
**Summary:** This article discusses one of the most effective ensemble learning techniques that integrate several decision trees to construct a classifier and regression tool. The method uses independently sampled random vectors to grow each tree as the trees vote for the most common class. Breiman notes that when the number of trees increases, then as the generalization error of the forest converges towards a stable value, the accuracy of the method improves due to the strength and low interdependence of individual classifiers. It works well with high dimensional data due to its efficiency even when the quantity of variables is many times greater than the observations. It also provides mechanisms to quantify the importance of the variables, hence, increasing its interpretative value. This work also compares the Random Forests with other methods such as Adaboost, illustrating the efficiency of the method in noisy and computational tasks. The article emphasizes the numerous theoretical and empirical investigations on the performance of Random Forests in various fields such as medical diagnostics, bioinformatics, and economics.[@breiman2001random]

**Title:** A Random Forest Guided Tour\
**Summary:** The paper explains thoroughly the Random Forest algorithm that Leo Breiman proposed in his 2001 research paper. The article covers the engineering and science development of the algorithm that has gained much popularity as an effective way of solving classification and regression problems. Furthermore, the article discusses the outstanding features of the algorithm, which include its processing of data with large dimensional ratios, meaning the number of variables is larger than the number of data points. Remarkable features of Random Forests which make the algorithm popular: the simplicity of use, reliability and effectiveness. The algorithm has a variety of applications in bioinformatics, ecology, and even 3D object recognition. The paper also explains fundamental aspects of Random Forests, which include parameter tuning, the random forests’ resampling methods, and the evaluation of importance of the features. The authors give informative comments on the functioning of the algorithm and suggest some bounds of relationships with algorithms using the nearest neighbor approach, kernel methods, and feature selection techniques. It is aimed to equip learners and professionals with sufficient knowledge about the merits and demerits of the algorithm along with its multifarious applications involving statistical learning.[@biau2016random]

**Title:** A Privacy-Preserving Algorithm for Clinical Decision-Support Systems Using Random Forest
**Summary:** The article proposes a unique solution for clinical decision-support systems (CDSS) by combining the use of the Random Forest (RF) algorithm as well as masking techniques. Their objective is the timely and efficient diagnosis of new patients’ symptoms while ensuring patient data is kept safe from possible cyber and network threats. In this article, the authors describe an approach for a domain-specific classification model for medical domains with the Privacy-Preserving Random Forest (PPRF) algorithm that enables hospitals to work together to create a global classification model for CDSS without revealing sensitive information. This algorithm automates the process of masking patient health records during data mining to guarantee confidentiality. Through simulations, it has been demonstrated that the efficiency and privacy protection offered by the proposed PPRF algorithm is superior to other existing models that claim to preserve the privacy of data by using attribute elimination and avoiding cryptographic processes. The proposed model has been developed with the use of hospital datasets to ensure that it meets the requirements for privacy of patient information and the resultant diagnosis. [@alabdulkarim2019privacy]

**Title:**Diabetes Prediction based on Machine Learning Algorithms: MNB, Random Forest, SVM\
**Summary:** In the article the authors examine the implementation of three machine learning algorithms, Multinomial Naive Bayes (MNB), Random Forest, and Support Vector Machine (SVM), to predict diabetes based on clinical and lifestyle data. This study was conducted using the Pima Indians Diabetes Dataset available at the National Institute of Diabetes and Digestive and Kidney Diseases. Several steps of Data Mining were undertaken, including the preprocessing of missing data and normalization of attributes of record to enhance the accuracy of the models. The study revealed that Random Forest produced the highest accuracy and robustness and was able to derive complex interactions among features while SVMs were highly effective in large-dimensional data. Simpler MNB was used as a baseline for comparison. The analysis revealed that both Random Forest and SVM are great candidates for early diabetes diagnosis as they accurately predict the disease, aiding the intervention plans prepared by the healthcare professionals. The article confirms the belief that the application of machine learning, particularly Random Forest and SVM, will improve the effectiveness of early diabetes detection and intervention, hence leading to improved management of healthcare.[@khine2022diabetes]

**Title:** Application of random forest model to predict the demand of essential medicines for non-communicable diseases management in public health facilities\
**Summary:** The paper deals with the attempt to utilize Random Forest (RF) models for predicting the requirement of essential drugs in the management of non-communicable diseases (NCDs) in public health facilities in Rwanda. This study seeks to improve the logistics of the healthcare system by maintaining stocks of essential medicines, so they do not run out, and resources can be managed better. The researchers collected consumption information of around 500 medical products from 2015 to 2019, out of which 17 NCD medicines that were most frequently used were selected. The RF model provided accurate demand forecasts for these medicines, achieving 78% accuracy in the training set and 71% in the testing set. While successful, the study points out that the generalizability of the model’s applicability is likely restricted to Rwanda, requiring more extensive investigation of its utility in different areas. The results of the study demonstrate the effectiveness of machine learning, especially RF models, in improving the forecasts needed for the healthcare systems and in public health management to enhance planning and operational effectiveness.[@mbonyinshuti2022demand]

**Title:** Random Forest Algorithms in Health Care Sectors: A Review of Applications\
**Summary:** The article is focused on the application of Random Forest algorithms in the branch of healthcare concerning predicting patient safety outcomes and analyzing the components of safety culture. The study points to the complex nature of safety culture in healthcare systems, in relation to medical negligence and patient safety incidents. The authors make use of aggregate data of hospitals in the United States, using Random Forest techniques to determine the correlation in the various organizational safety components and the patient safety grades. Their findings indicate that regarding protective measures towards patients, healthcare quality knowledge possesses an important component along with organizational factors and top management objectives. This particular study findings have also emphasized the gap of knowledge concerning which of the components of safety culture are the most influential regarding the safety outcomes of patients, thus underscoring the need for additional research that would make these findings useful in other healthcare systems. .[@probst2019rfhealth]

------------------------------------------------------------------------

## Maddie's Literature Review

**Title:** Random Forest in Insurance Medicine\
**Summary:** The goal of this paper is to explore and describe the random forest technique. It provides an overview of the decision tree process and how random forests are created using decision trees. The explanations in the article are very useful for understanding the process. It goes into detail about CART models, how a decision tree is formed using randomization techniques such as bootstrapping and bagging, and then randomization at each node. The article briefly explains being able to tune the model with number of splits and number of trees, which affect the computational intensity. The article then focuses on using a random forest to analyze survival data. Colon cancer data is used to create models using both random forest and Cox, and results are found to be comparable. The main drawback the author writes of the random forest as compared to Cox, is that it doesn't give insights into partial effects of the predictors, making it much more difficult to understand how the predictors may individually or collectively be influencing the prediction. [@rigatti2017random]

**Title:** How many trees in a random forest?\
**Summary:**The purpose of this article is to research whether there is an optimal number of trees within a random forest. The goal is to determine if there's a threshold in which increasing the number of trees provides no significant performance gain as compared to the increase of computational cost. The article states that in general, the user sets the number of trees on a trial and error basis. They review the results from an analysis of increasing the number of trees, between 2 and 4096, doubling the number of trees at every iteration for 29 data sets. The number of attributes within each random forest is also analyzed with the growth of trees. Results were analyzed using ROC curve (AUC) and the percentage of attributes used. In general, as the number of trees increased, so did the AUC. However, there was no significant difference between the given number of trees and double that number. We needed to do at least 4x the number to see a significant improvement. There was no significant difference between 128 trees until 4096 trees. The article suggests that based on their experiments, a range between 64 and 128 trees in a forest tends to be an optimal balance between AUC, processing time, and memory usage. [@oshiro2012many]

**Title:** Using Random Forest Algorithm for Breast Cancer Diagnosis\
**Summary:** The purpose of this paper is to analyze the diagnosis of breast cancer using the random forest algorithm. The article begins with giving a basic background of the algorithm and how it functions. It then breaks down into sections further detailing the different steps and aspects of the random forest, such as bagging sampling and decision tree construction. “That is to say, in each round of random sampling of bagging, about 36.8% of the data in the training set is not collected by the sample set.” This is referred to as the Out Of Bag data, which is then used for testing the model. The data set used in the paper is the University of California, Irvine Breast Cancer Wisconsin (Diagnostic) Dataset, which has 569 medical records of multidimensional data. The article describes the different fields that are in the data set and does some basic proportional and correlation analysis. The author was able to achieve approximately 95% accuracy on the model and ROC OOB of 99.3%. This shows that the random forest can be a great choice in diagnosis prediction by producing classification results. [@dai2018using]

**Title:** Hyperparameters and Tuning Strategies for Random Forest\
**Summary:** The purpose of this article is to examine the hyperparameters and tuning strategies for random forests. It reviews the various hyperparameters that must be set (i.e. number of trees, number of observations drawn randomly for each tree, replacement, number of variables for each split, etc). There are defaults for all of these hyperparameters if the user does not specify, however, the purpose of tuning these hyperparameters is to achieve optimal results for the specific data set being utilized. The article breaks down the hyperparameter analysis into sections – influence on performance and influence on variable importance; with subsection for each hyperparameter. They state that an optimal compromise between low correlation and reasonable strength of the trees has to be found, which can be controlled by the parameters mtry, sample size, and node size. The article did a great job diving into the different hyperparameters, explaining their purpose, why typical default values are what they are, and how choosing different values can affect the models. The article discusses how random forests are generally known to provide good results using the default settings, and tuning of hyperparameters is overall much less necessary/beneficial than other algorithms. It reviews various R packages and functions that can be used for hyperparameter tuning and how each function works – such as tuneRF which calculates the out-of-bag error with the default mtry value, and then tries smaller or larger values until there is no more improvement and returns the model with the best value. The authors also delve into the details of a tuning package they created, tuneRanger, and compare it to existing R packages. Overall, it exceeded in nearly every category – MMCE, AUC, Brier score, Logarithmic loss, and Training runtime. The variable mtry has shown to have the most potential when tuning a random forest model.[@probst2019hyperparameters]

**Title:** Predicting Disease Risks from Highly Imbalanced Data Using Random Forest\
**Summary:** The purpose of this article is to explore predicting different diseases using random forest when the data set is highly imbalanced. “Class imbalance occurs if one class contains significantly more samples than the other class. Since the classification process assumes that the data is drawn from the same distribution as the training data, presenting imbalanced data to the classifier will produce undesirable results.” The data set being used is from the Nationwide Inpatient Sample (NIS) and has approximately 8 million records of hospital stays and 126 data elements for each stay. A potential hurdle that was mentioned in the article is that there is no patient identifier in the data set, therefore no way to determine if multiple records belong to the same patient or if time elapsed between diagnoses. The author describes the pre-processing that was necessary for the data set, such as parsing the data as it was not delimited. They describe how repeated random sub-sampling was effective for handling the highly imbalanced data set. In this, they partition the data set into sub-samples, each containing an equal number of instances from each class. “RF uses the Gini measure of impurity to select the split with the lowest impurity at every node.” The author discusses variable important in random forests, which they define as “Variable importance measures the degree of association between a given variable and the classification result. RF has four measures for the variable importance: raw importance score for class 0, raw importance score for class 1, decrease in accuracy and the Gini index”. For their model evaluation, they compared the results against SVM, boosting, and bagging; using ROC and AUC for the comparison. They found that on 7 of the 8 categories tested against, RF performed the best. They compared results using sampling and non-sampling and found that the classifications using sampling performed better in both accuracy and speed. The average RF AUC across all categories was 89.05%. [@khalilia2011predicting]

**Title:** Overview of Random Forest Methodology and Practical Guidance\
**Summary:**The purpose of this article is to provide an overview of the random forest algorithm, with a specific emphasis on applications to bioinformatics and computational biology. One item that was discussed was VIMs, or variable importance measures. They claim that “Random forest (RF) methodology is used to address two main classes of problems: to construct a prediction rule in a supervised learning problem and to assess and rank variables with respect to their ability to predict the response.” Variable importance measures, or VIMs, are automatically computed for each predictor in the random forest algorithm and can be used to identify predictors that are involved in interactions. Since random forest works well with high-dimensional data, it is often appropriate for clinical or bioinformatics use cases. The author describes the basics of how the random forest algorithm functions, and then brings up the standard method has an important pitfall: “In the split selection process, predictors may be favored or disfavored depending on their scale of measurement or, in the case of categorical predictors, on their number of categories.” The author further describes this pitfall in the article, and how it can lead to bias. The author describes another class of decision trees, called conditional inference forests (CIF), which addresses the issue found in the standard random forest. The author brings up how if there’s a large number of predictors, then the default value of sqrt(p) or p/3 may be too small, as in their case it happened to randomly select predictors that were all noninformative, resulting in trees that weren’t accurate. The article reviews the use cases of RF in genetic epidemiology, and using SNP’s (single nucleotide polymorphism) as predictors, and shows an analysis using SNP data to demonstrate how the Gini VIM favored uncorrelated SNPs over strongly correlated SNPs, even if they were all noninformantive; therefore it was recommended to use permutation VIM. In conclusion, the RF algorithm is a useful tool for prediction, but does have its setbacks such as producing less than desirable results stemmed from bias or poor distribution of the predictor.[@boulesteix2012overview]
